{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "02f68aee-5eaa-49ce-94e9-f9bfda07d023",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: torch in /opt/sw/spack/apps/linux-rhel8-x86_64_v2/gcc-10.3.0/python-3.9.9-jh/lib/python3.9/site-packages (1.12.1+cu113)\n",
      "Requirement already satisfied: typing-extensions in /home/sdandibh/.local/lib/python3.9/site-packages (from torch) (4.8.0)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: transformers in /home/sdandibh/.local/lib/python3.9/site-packages (4.35.2)\n",
      "Requirement already satisfied: filelock in /opt/sw/spack/apps/linux-rhel8-x86_64_v2/gcc-10.3.0/python-3.9.9-jh/lib/python3.9/site-packages (from transformers) (3.6.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /home/sdandibh/.local/lib/python3.9/site-packages (from transformers) (0.19.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/sw/spack/apps/linux-rhel8-x86_64_v2/gcc-10.3.0/python-3.9.9-jh/lib/python3.9/site-packages (from transformers) (1.22.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/sw/spack/apps/linux-rhel8-x86_64_v2/gcc-10.3.0/python-3.9.9-jh/lib/python3.9/site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/sw/spack/apps/linux-rhel8-x86_64_v2/gcc-10.3.0/python-3.9.9-jh/lib/python3.9/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/sw/spack/apps/linux-rhel8-x86_64_v2/gcc-10.3.0/python-3.9.9-jh/lib/python3.9/site-packages (from transformers) (2022.3.15)\n",
      "Requirement already satisfied: requests in /opt/sw/spack/apps/linux-rhel8-x86_64_v2/gcc-10.3.0/python-3.9.9-jh/lib/python3.9/site-packages (from transformers) (2.27.1)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /home/sdandibh/.local/lib/python3.9/site-packages (from transformers) (0.15.0)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /home/sdandibh/.local/lib/python3.9/site-packages (from transformers) (0.4.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/sw/spack/apps/linux-rhel8-x86_64_v2/gcc-10.3.0/python-3.9.9-jh/lib/python3.9/site-packages (from transformers) (4.63.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/sdandibh/.local/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/sdandibh/.local/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.8.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/sw/spack/apps/linux-rhel8-x86_64_v2/gcc-10.3.0/python-3.9.9-jh/lib/python3.9/site-packages (from packaging>=20.0->transformers) (3.0.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/sw/spack/apps/linux-rhel8-x86_64_v2/gcc-10.3.0/python-3.9.9-jh/lib/python3.9/site-packages (from requests->transformers) (1.26.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/sw/spack/apps/linux-rhel8-x86_64_v2/gcc-10.3.0/python-3.9.9-jh/lib/python3.9/site-packages (from requests->transformers) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/sw/spack/apps/linux-rhel8-x86_64_v2/gcc-10.3.0/python-3.9.9-jh/lib/python3.9/site-packages (from requests->transformers) (2.0.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/sw/spack/apps/linux-rhel8-x86_64_v2/gcc-10.3.0/python-3.9.9-jh/lib/python3.9/site-packages (from requests->transformers) (3.3)\n"
     ]
    }
   ],
   "source": [
    "# Install the required libraries\n",
    "!pip install torch\n",
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "582710ab-3925-4a17-974d-985b9bf0d1b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Love is a vast sea of knowledge, and all too true is the desire to know more, to know more and more, to find more and more, to know more and more, to find more and to find more, that is, to know more and more, to find more and to find more, that is, to find more and to find more, and to find more and to find more, and to find more and to find more, and to find more and to find more,\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Define the model name\n",
    "model_name = \"Chetan007/gpt2-sonnet-generators\"\n",
    "\n",
    "# Create a text generation pipeline\n",
    "generator = pipeline('text-generation', model=model_name, device=0)  # Set device=0 for GPU, or device=-1 for CPU\n",
    "\n",
    "# Generate a sonnet\n",
    "sonnet = generator(\"Love is a vast sea\", max_length=100, num_return_sequences=1)[0]['generated_text']\n",
    "\n",
    "# Print the generated sonnet\n",
    "print(sonnet)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9e3a1919-ea64-4f40-9b8a-5c930fa7d10c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 39/39 [00:01<00:00, 20.88it/s]\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Sonnet:\n",
      "Love is as fair as a flower is fair, whereon thy fair face doth lie, that thou mayst take delight in, and in that which thou dost decease:   But wherefore do I not say, 'If thou dost not, I do not approve of my mistress' fair complexion'?  I grant, she is thy mistress, though I never saw her in my life.  And yet I dare not tell her my name, lest she should know that I\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import GPTNeoForCausalLM, GPT2Tokenizer\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "import csv\n",
    "\n",
    "# Define the GPT-Neo model and tokenizer\n",
    "model_name = \"Chetan007/gpt2-sonnet-generators\"\n",
    "model = GPTNeoForCausalLM.from_pretrained(model_name)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Set device (GPU if available, otherwise CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Define a custom dataset to load sonnets from a CSV file\n",
    "class SonnetDataset(Dataset):\n",
    "    def __init__(self, csv_file, tokenizer, max_length=128):\n",
    "        self.sonnets = self.load_sonnets(csv_file)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def load_sonnets(self, csv_file):\n",
    "        sonnets = []\n",
    "        with open(csv_file, 'r', encoding='utf-8') as file:\n",
    "            reader = csv.reader(file)\n",
    "            for row in reader:\n",
    "                sonnet = row[0].strip()  # Assuming sonnet is in the first column\n",
    "                if sonnet:\n",
    "                    sonnets.append(sonnet)\n",
    "        return sonnets\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sonnets)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sonnet = self.sonnets[idx]\n",
    "        encoding = self.tokenizer(sonnet, return_tensors=\"pt\", truncation=True, max_length=self.max_length, padding=\"max_length\")\n",
    "        return {key: encoding[key][0] for key in encoding}\n",
    "\n",
    "# Set up your dataset and DataLoader\n",
    "csv_file_path = \"shakespeare_sonnets.csv\"\n",
    "sonnet_dataset = SonnetDataset(csv_file_path, tokenizer)\n",
    "train_loader = DataLoader(sonnet_dataset, batch_size=4, shuffle=True)\n",
    "\n",
    "# Set up training parameters\n",
    "num_epochs = 1\n",
    "learning_rate = 5e-5\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "    for batch in tqdm(train_loader, desc=\"Training\"):\n",
    "        inputs = {key: batch[key].to(device) for key in batch}\n",
    "        outputs = model(**inputs, labels=inputs[\"input_ids\"])\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "# Generate a sonnet\n",
    "prompt = \"Love is as fair as a flower\"\n",
    "input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
    "output = model.generate(input_ids, max_length=100, num_beams=5, no_repeat_ngram_size=2, top_k=50, top_p=0.95, temperature=0.8)\n",
    "\n",
    "# Decode and print the generated sonnet\n",
    "generated_sonnet = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "print(\"Generated Sonnet:\")\n",
    "print(generated_sonnet)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "adb69de4-13b8-4c30-a5e5-43269d8f1480",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 20/20 [00:01<00:00, 13.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 20/20 [00:01<00:00, 14.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 20/20 [00:01<00:00, 14.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 20/20 [00:01<00:00, 14.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 20/20 [00:01<00:00, 14.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 20/20 [00:01<00:00, 14.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 20/20 [00:01<00:00, 14.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 20/20 [00:01<00:00, 14.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 20/20 [00:01<00:00, 14.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 20/20 [00:01<00:00, 14.31it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('finetuned_model.p/tokenizer_config.json',\n",
       " 'finetuned_model.p/special_tokens_map.json',\n",
       " 'finetuned_model.p/vocab.json',\n",
       " 'finetuned_model.p/merges.txt',\n",
       " 'finetuned_model.p/added_tokens.json')"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import GPTNeoForCausalLM, GPT2Tokenizer\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "import csv\n",
    "\n",
    "# Define the GPT-Neo model and tokenizer\n",
    "model_name = \"Chetan007/gpt2-sonnet-generators\"\n",
    "model = GPTNeoForCausalLM.from_pretrained(model_name)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "\n",
    "\n",
    "# Set pad_token_id to eos_token_id\n",
    "model.config.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "# Set device (GPU if available, otherwise CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Define a custom dataset to load sonnets from a CSV file\n",
    "class SonnetDataset(Dataset):\n",
    "    def __init__(self, csv_file, tokenizer, max_length=128):\n",
    "        self.sonnets = self.load_sonnets(csv_file)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def load_sonnets(self, csv_file):\n",
    "        sonnets = []\n",
    "        with open(csv_file, 'r', encoding='utf-8') as file:\n",
    "            reader = csv.reader(file)\n",
    "            for row in reader:\n",
    "                sonnet = row[0].strip()  # Assuming sonnet is in the first column\n",
    "                if sonnet:\n",
    "                    sonnets.append(sonnet)\n",
    "        return sonnets\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sonnets)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sonnet = self.sonnets[idx]\n",
    "        encoding = self.tokenizer(sonnet, return_tensors=\"pt\", truncation=True, max_length=self.max_length, padding=\"max_length\")\n",
    "        return {key: encoding[key][0] for key in encoding}\n",
    "\n",
    "# Set up your dataset and DataLoader\n",
    "csv_file_path = \"shakespeare_sonnets.csv\"\n",
    "sonnet_dataset = SonnetDataset(csv_file_path, tokenizer)\n",
    "train_loader = DataLoader(sonnet_dataset, batch_size=8, shuffle=True)\n",
    "\n",
    "# Set up training parameters\n",
    "num_epochs = 10\n",
    "learning_rate = 5e-5\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.95)\n",
    "\n",
    "# Training loop\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "    for batch in tqdm(train_loader, desc=\"Training\"):\n",
    "        inputs = {key: batch[key].to(device) for key in batch}\n",
    "        outputs = model(**inputs, labels=inputs[\"input_ids\"])\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "    scheduler.step()\n",
    "    \n",
    "# Save the trained model\n",
    "output_model_path = \"finetuned_model.p\"\n",
    "model.save_pretrained(output_model_path)\n",
    "tokenizer.save_pretrained(output_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "50c9162c-9727-4018-99af-b21f4c379c77",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting syllables\n",
      "  Downloading syllables-1.0.9-py3-none-any.whl.metadata (2.4 kB)\n",
      "Requirement already satisfied: cmudict<2.0.0,>=1.0.11 in /home/sdandibh/.local/lib/python3.9/site-packages (from syllables) (1.0.15)\n",
      "Requirement already satisfied: importlib-metadata<7.0,>=5.1 in /home/sdandibh/.local/lib/python3.9/site-packages (from syllables) (6.8.0)\n",
      "Requirement already satisfied: importlib-resources<6.0.0,>=5.10.1 in /home/sdandibh/.local/lib/python3.9/site-packages (from cmudict<2.0.0,>=1.0.11->syllables) (5.13.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/sw/spack/apps/linux-rhel8-x86_64_v2/gcc-10.3.0/python-3.9.9-jh/lib/python3.9/site-packages (from importlib-metadata<7.0,>=5.1->syllables) (3.7.0)\n",
      "Downloading syllables-1.0.9-py3-none-any.whl (15 kB)\n",
      "Installing collected packages: syllables\n",
      "Successfully installed syllables-1.0.9\n"
     ]
    }
   ],
   "source": [
    "!pip install syllables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e1eba247-1d22-4b77-871c-9ef4d5864bf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Formatted Sonnet:\n",
      "Love is a fair flower, which in meadows\n",
      "And sweetpied flowers in the brocade\n",
      "Me, and sweet with you. o, what sweetest you,\n",
      "You are, when i first laid my hand on\n",
      "And all alone heaven's sweet pride\n",
      "My newfangledies and put them in my\n",
      "You have grown so fond of me then you must\n",
      "Grow so PAD PAD PAD PAD PAD PAD PAD PAD\n"
     ]
    }
   ],
   "source": [
    "# Generate a sonnet\n",
    "prompt = \"Love is a fair flower\"\n",
    "input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
    "output = model.generate(input_ids, max_length=100, num_beams=5, no_repeat_ngram_size=2, top_k=50, top_p=0.95, temperature=0.2)\n",
    "\n",
    "import syllables\n",
    "\n",
    "def post_process_sonnet(generated_sonnet):\n",
    "    # Split the text into lines based on syllable count (10 syllables per line)\n",
    "    syllables_per_line = 10\n",
    "    words = generated_sonnet.split()\n",
    "    lines = [\" \".join(words[i:i+syllables_per_line]) for i in range(0, len(words), syllables_per_line)]\n",
    "\n",
    "    # Capitalize the first word of each line\n",
    "    lines = [line.capitalize() for line in lines]\n",
    "\n",
    "    # Ensure each line has exactly 10 syllables\n",
    "    for i, line in enumerate(lines):\n",
    "        current_syllables = sum(syllables.estimate(word) for word in line.split())\n",
    "        if current_syllables > syllables_per_line:\n",
    "            # Truncate words if the line has more than 10 syllables\n",
    "            words_in_line = line.split()\n",
    "            while current_syllables > syllables_per_line:\n",
    "                # Remove the last word until the line has exactly 10 syllables\n",
    "                last_word = words_in_line.pop()\n",
    "                current_syllables -= syllables.estimate(last_word)\n",
    "            lines[i] = \" \".join(words_in_line)\n",
    "        elif current_syllables < syllables_per_line:\n",
    "            # Pad words if the line has fewer than 10 syllables\n",
    "            lines[i] = line + \" \" + \" \".join([\"<PAD>\" for _ in range(syllables_per_line - current_syllables)])\n",
    "\n",
    "    # Filter out unwanted characters and remove empty lines\n",
    "    allowed_characters = set(\"abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ',.?! \")\n",
    "    lines = [\"\".join(c for c in line if c in allowed_characters) for line in lines]\n",
    "    lines = [line for line in lines if line]  # Remove empty lines\n",
    "\n",
    "    # Join the lines to form the sonnet\n",
    "    formatted_sonnet = \"\\n\".join(lines)\n",
    "\n",
    "    return formatted_sonnet\n",
    "\n",
    "# Decode and print the generated sonnet\n",
    "generated_sonnet = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "formatted_sonnet = post_process_sonnet(generated_sonnet)\n",
    "print(\"Formatted Sonnet:\")\n",
    "print(formatted_sonnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "78cf5637-30fc-470e-a608-a1999d8b319e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sdandibh/.local/lib/python3.9/site-packages/transformers/generation/utils.py:1473: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Sonnet:\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('finetuned_model_sonnet.p/tokenizer_config.json',\n",
       " 'finetuned_model_sonnet.p/special_tokens_map.json',\n",
       " 'finetuned_model_sonnet.p/vocab.json',\n",
       " 'finetuned_model_sonnet.p/merges.txt',\n",
       " 'finetuned_model_sonnet.p/added_tokens.json')"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import syllables\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Generate sonnet\n",
    "generated_lines = []\n",
    "for _ in range(14):  # Generate 14 lines\n",
    "    prompt = \"Love\"  # You can change the prompt as needed\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    # Ensure attention_mask is set\n",
    "    attention_mask = torch.ones(input_ids.shape, device=device)\n",
    "\n",
    "    output = model.generate(\n",
    "        input_ids,\n",
    "        attention_mask=attention_mask,  # Pass attention_mask\n",
    "        max_length=100,  # Adjust as needed\n",
    "        num_beams=5,\n",
    "        no_repeat_ngram_size=2,\n",
    "        top_k=50,\n",
    "        top_p=1.0,  # Set to 1.0 for greedy decoding\n",
    "        temperature=1.0  # Set to 1.0 for greedy decoding\n",
    "    )\n",
    "\n",
    "    # Decode and format sonnet\n",
    "    sonnet_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    sonnet_lines = sonnet_text.split(\"\\n\")\n",
    "\n",
    "    # Filter out lines with syllable count other than 10\n",
    "    sonnet_lines = [line.strip() for line in sonnet_lines if syllables.estimate(line) == 10]\n",
    "\n",
    "    generated_lines.extend(sonnet_lines)\n",
    "\n",
    "# Join the lines to form the sonnet\n",
    "formatted_sonnet = \"\\n\".join(generated_lines)\n",
    "\n",
    "# Print and save the generated sonnet\n",
    "print(\"Generated Sonnet:\")\n",
    "print(formatted_sonnet)\n",
    "\n",
    "# Save the trained model\n",
    "output_model_path = \"finetuned_model_sonnet.p\"\n",
    "model.save_pretrained(output_model_path)\n",
    "tokenizer.save_pretrained(output_model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "58a7cb86-fdb8-4947-af6a-8e81d5812a45",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type gpt_neo to instantiate a model of type gpt2. This is not supported for all configurations of models and can yield errors.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The following `model_kwargs` are not used by the model: ['config'] (note: typos in the generate arguments will also show up in this list)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [45]\u001b[0m, in \u001b[0;36m<cell line: 18>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Ensure attention_mask is set\u001b[39;00m\n\u001b[1;32m     23\u001b[0m attention_mask \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mones(input_ids\u001b[38;5;241m.\u001b[39mshape, device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[0;32m---> 25\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Pass attention_mask\u001b[39;49;00m\n\u001b[1;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Adjust as needed\u001b[39;49;00m\n\u001b[1;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_beams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m    \u001b[49m\u001b[43mno_repeat_ngram_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Set to 1.0 for greedy decoding\u001b[39;49;00m\n\u001b[1;32m     33\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Set to 1.0 for greedy decoding\u001b[39;49;00m\n\u001b[1;32m     34\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Pass the GPT2Config for generation\u001b[39;49;00m\n\u001b[1;32m     35\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m# Decode and format sonnet\u001b[39;00m\n\u001b[1;32m     38\u001b[0m sonnet_text \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mdecode(output[\u001b[38;5;241m0\u001b[39m], skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m/opt/sw/spack/apps/linux-rhel8-x86_64_v2/gcc-10.3.0/python-3.9.9-jh/lib/python3.9/site-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclone():\n\u001b[0;32m---> 27\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/transformers/generation/utils.py:1485\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1483\u001b[0m model_kwargs \u001b[38;5;241m=\u001b[39m generation_config\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# All unused kwargs must be model kwargs\u001b[39;00m\n\u001b[1;32m   1484\u001b[0m generation_config\u001b[38;5;241m.\u001b[39mvalidate()\n\u001b[0;32m-> 1485\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_model_kwargs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1487\u001b[0m \u001b[38;5;66;03m# 2. Set generation parameters if not already defined\u001b[39;00m\n\u001b[1;32m   1488\u001b[0m logits_processor \u001b[38;5;241m=\u001b[39m logits_processor \u001b[38;5;28;01mif\u001b[39;00m logits_processor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m LogitsProcessorList()\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/transformers/generation/utils.py:1262\u001b[0m, in \u001b[0;36mGenerationMixin._validate_model_kwargs\u001b[0;34m(self, model_kwargs)\u001b[0m\n\u001b[1;32m   1259\u001b[0m         unused_model_args\u001b[38;5;241m.\u001b[39mappend(key)\n\u001b[1;32m   1261\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m unused_model_args:\n\u001b[0;32m-> 1262\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1263\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe following `model_kwargs` are not used by the model: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00munused_model_args\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (note: typos in the\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1264\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m generate arguments will also show up in this list)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1265\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: The following `model_kwargs` are not used by the model: ['config'] (note: typos in the generate arguments will also show up in this list)"
     ]
    }
   ],
   "source": [
    "from transformers import GPTNeoForCausalLM, GPT2Tokenizer, GPT2Config\n",
    "import torch\n",
    "\n",
    "# Load GPT-Neo model and tokenizer\n",
    "model_name = \"Chetan007/gpt2-sonnet-generators\"\n",
    "model = GPTNeoForCausalLM.from_pretrained(model_name)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Set device (GPU if available, otherwise CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Set up a GPT2Config object for generation\n",
    "generation_config = GPT2Config.from_pretrained(model_name)\n",
    "\n",
    "# Generate sonnet\n",
    "generated_lines = []\n",
    "for _ in range(14):  # Generate 14 lines\n",
    "    prompt = \"Love\"  # You can change the prompt as needed\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    # Ensure attention_mask is set\n",
    "    attention_mask = torch.ones(input_ids.shape, device=device)\n",
    "\n",
    "    output = model.generate(\n",
    "        input_ids,\n",
    "        attention_mask=attention_mask,  # Pass attention_mask\n",
    "        max_length=100,  # Adjust as needed\n",
    "        num_beams=5,\n",
    "        no_repeat_ngram_size=2,\n",
    "        top_k=50,\n",
    "        top_p=1.0,  # Set to 1.0 for greedy decoding\n",
    "        temperature=1.0,  # Set to 1.0 for greedy decoding\n",
    "        config=generation_config  # Pass the GPT2Config for generation\n",
    "    )\n",
    "\n",
    "    # Decode and format sonnet\n",
    "    sonnet_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    sonnet_lines = sonnet_text.split(\"\\n\")\n",
    "\n",
    "    # Filter out lines with a syllable count other than 10\n",
    "    sonnet_lines = [line.strip() for line in sonnet_lines if syllables.estimate(line) == 10]\n",
    "\n",
    "    generated_lines.extend(sonnet_lines)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "90668b0e-9b46-4264-acad-ef82f5d70c73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 20/20 [00:01<00:00, 13.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 20/20 [00:01<00:00, 14.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 20/20 [00:01<00:00, 14.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 20/20 [00:01<00:00, 14.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 20/20 [00:01<00:00, 14.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 20/20 [00:01<00:00, 14.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 20/20 [00:01<00:00, 14.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 20/20 [00:01<00:00, 14.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 20/20 [00:01<00:00, 14.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 20/20 [00:01<00:00, 14.43it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('finetuned_model/tokenizer_config.json',\n",
       " 'finetuned_model/special_tokens_map.json',\n",
       " 'finetuned_model/vocab.json',\n",
       " 'finetuned_model/merges.txt',\n",
       " 'finetuned_model/added_tokens.json')"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, GPT2Config\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "import csv\n",
    "\n",
    "# Define the GPT-2 model and tokenizer\n",
    "model_name = \"gpt2\"\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token  # Set pad_token to eos_token\n",
    "\n",
    "\n",
    "# Set device (GPU if available, otherwise CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Define a custom dataset to load sonnets from a CSV file\n",
    "class SonnetDataset(Dataset):\n",
    "    def __init__(self, csv_file, tokenizer, max_length=128):\n",
    "        self.sonnets = self.load_sonnets(csv_file)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def load_sonnets(self, csv_file):\n",
    "        sonnets = []\n",
    "        with open(csv_file, 'r', encoding='utf-8') as file:\n",
    "            reader = csv.reader(file)\n",
    "            for row in reader:\n",
    "                sonnet = row[0].strip()  # Assuming sonnet is in the first column\n",
    "                if sonnet:\n",
    "                    sonnets.append(sonnet)\n",
    "        return sonnets\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sonnets)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sonnet = self.sonnets[idx]\n",
    "        encoding = self.tokenizer(sonnet, return_tensors=\"pt\", truncation=True, max_length=self.max_length, padding=\"max_length\")\n",
    "        return {key: encoding[key][0] for key in encoding}\n",
    "\n",
    "# Set up your dataset and DataLoader\n",
    "csv_file_path = \"shakespeare_sonnets.csv\"  # Adjust the path to your sonnet dataset\n",
    "sonnet_dataset = SonnetDataset(csv_file_path, tokenizer)\n",
    "train_loader = DataLoader(sonnet_dataset, batch_size=8, shuffle=True)\n",
    "\n",
    "# Set up training parameters\n",
    "num_epochs = 10\n",
    "learning_rate = 5e-5\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.95)\n",
    "\n",
    "# Training loop\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "    for batch in tqdm(train_loader, desc=\"Training\"):\n",
    "        inputs = {key: batch[key].to(device) for key in batch}\n",
    "        outputs = model(**inputs, labels=inputs[\"input_ids\"])\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "    scheduler.step()\n",
    "\n",
    "# Save the trained model\n",
    "output_model_path = \"finetuned_model\"\n",
    "model.save_pretrained(output_model_path)\n",
    "tokenizer.save_pretrained(output_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "5ffc96fb-ff00-412f-9a88-e7fe4619f660",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Formatted Sonnet:\n",
      "Love is a fair flower, but not so sweet\n",
      "Thy sweetest buds, which, when ripe, are\n",
      "Is thy beauty so great as mine, nor\n",
      "Nor my own worth so high as theirs but thou,\n",
      "Thyself, my love'st self, thyself's\n",
      "Of all fairies. o, if i may say so,\n",
      "Love thee more than i do love thy self.' 'tis\n",
      "True,' i say, 'tis true' PAD PAD PAD PAD PAD\n"
     ]
    }
   ],
   "source": [
    "# Generate a sonnet using the fine-tuned model with TextGenerationConfig\n",
    "config = GPT2Config.from_pretrained(output_model_path)\n",
    "config.max_length = 14 * 10  # 14 lines with 10 syllables each\n",
    "config.num_return_sequences = 1  # Number of sequences to generate\n",
    "model = GPT2LMHeadModel.from_pretrained(output_model_path, config=config)\n",
    "model.to(device)  # Move the model back to the device\n",
    "\n",
    "# Generate a sonnet using the fine-tuned model\n",
    "prompt = \"Love is a fair flower\"\n",
    "input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
    "output = model.generate(input_ids, max_length=config.max_length, num_beams=5, no_repeat_ngram_size=2, top_k=50, top_p=0.95, temperature=0.2)\n",
    "\n",
    "import syllables\n",
    "\n",
    "def post_process_sonnet(generated_sonnet):\n",
    "    # Split the text into lines based on syllable count (10 syllables per line)\n",
    "    syllables_per_line = 10\n",
    "    words = generated_sonnet.split()\n",
    "    lines = [\" \".join(words[i:i+syllables_per_line]) for i in range(0, len(words), syllables_per_line)]\n",
    "\n",
    "    # Capitalize the first word of each line\n",
    "    lines = [line.capitalize() for line in lines]\n",
    "\n",
    "    # Ensure each line has exactly 10 syllables\n",
    "    for i, line in enumerate(lines):\n",
    "        current_syllables = sum(syllables.estimate(word) for word in line.split())\n",
    "        if current_syllables > syllables_per_line:\n",
    "            # Truncate words if the line has more than 10 syllables\n",
    "            words_in_line = line.split()\n",
    "            while current_syllables > syllables_per_line:\n",
    "                # Remove the last word until the line has exactly 10 syllables\n",
    "                last_word = words_in_line.pop()\n",
    "                current_syllables -= syllables.estimate(last_word)\n",
    "            lines[i] = \" \".join(words_in_line)\n",
    "        elif current_syllables < syllables_per_line:\n",
    "            # Pad words if the line has fewer than 10 syllables\n",
    "            lines[i] = line + \" \" + \" \".join([\"<PAD>\" for _ in range(syllables_per_line - current_syllables)])\n",
    "\n",
    "    # Filter out unwanted characters and remove empty lines\n",
    "    allowed_characters = set(\"abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ',.?! \")\n",
    "    lines = [\"\".join(c for c in line if c in allowed_characters) for line in lines]\n",
    "    lines = [line for line in lines if line]  # Remove empty lines\n",
    "\n",
    "    # Join the lines to form the sonnet\n",
    "    formatted_sonnet = \"\\n\".join(lines)\n",
    "\n",
    "    return formatted_sonnet\n",
    "\n",
    "# Decode and print the generated sonnet\n",
    "generated_sonnet = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "formatted_sonnet = post_process_sonnet(generated_sonnet)\n",
    "print(\"Formatted Sonnet:\")\n",
    "print(formatted_sonnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f44b6082-a729-47fb-8499-261f759d5d5b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
