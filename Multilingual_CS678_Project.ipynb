{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b65b4a06-afa8-4684-870b-c8f59675e4a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: torch in /opt/sw/spack/apps/linux-rhel8-x86_64_v2/gcc-10.3.0/python-3.9.9-jh/lib/python3.9/site-packages (1.12.1+cu113)\n",
      "Requirement already satisfied: typing-extensions in /home/sdandibh/.local/lib/python3.9/site-packages (from torch) (4.8.0)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: transformers in /home/sdandibh/.local/lib/python3.9/site-packages (4.35.2)\n",
      "Requirement already satisfied: filelock in /opt/sw/spack/apps/linux-rhel8-x86_64_v2/gcc-10.3.0/python-3.9.9-jh/lib/python3.9/site-packages (from transformers) (3.6.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /home/sdandibh/.local/lib/python3.9/site-packages (from transformers) (0.19.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/sw/spack/apps/linux-rhel8-x86_64_v2/gcc-10.3.0/python-3.9.9-jh/lib/python3.9/site-packages (from transformers) (1.22.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/sw/spack/apps/linux-rhel8-x86_64_v2/gcc-10.3.0/python-3.9.9-jh/lib/python3.9/site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/sw/spack/apps/linux-rhel8-x86_64_v2/gcc-10.3.0/python-3.9.9-jh/lib/python3.9/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/sw/spack/apps/linux-rhel8-x86_64_v2/gcc-10.3.0/python-3.9.9-jh/lib/python3.9/site-packages (from transformers) (2022.3.15)\n",
      "Requirement already satisfied: requests in /opt/sw/spack/apps/linux-rhel8-x86_64_v2/gcc-10.3.0/python-3.9.9-jh/lib/python3.9/site-packages (from transformers) (2.27.1)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /home/sdandibh/.local/lib/python3.9/site-packages (from transformers) (0.15.0)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /home/sdandibh/.local/lib/python3.9/site-packages (from transformers) (0.4.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/sw/spack/apps/linux-rhel8-x86_64_v2/gcc-10.3.0/python-3.9.9-jh/lib/python3.9/site-packages (from transformers) (4.63.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/sdandibh/.local/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/sdandibh/.local/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.8.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/sw/spack/apps/linux-rhel8-x86_64_v2/gcc-10.3.0/python-3.9.9-jh/lib/python3.9/site-packages (from packaging>=20.0->transformers) (3.0.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/sw/spack/apps/linux-rhel8-x86_64_v2/gcc-10.3.0/python-3.9.9-jh/lib/python3.9/site-packages (from requests->transformers) (1.26.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/sw/spack/apps/linux-rhel8-x86_64_v2/gcc-10.3.0/python-3.9.9-jh/lib/python3.9/site-packages (from requests->transformers) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/sw/spack/apps/linux-rhel8-x86_64_v2/gcc-10.3.0/python-3.9.9-jh/lib/python3.9/site-packages (from requests->transformers) (2.0.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/sw/spack/apps/linux-rhel8-x86_64_v2/gcc-10.3.0/python-3.9.9-jh/lib/python3.9/site-packages (from requests->transformers) (3.3)\n"
     ]
    }
   ],
   "source": [
    "# Install the required libraries\n",
    "!pip install torch\n",
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6f87818f-cc79-469b-9992-c2585f1fe89c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, GPT2Config\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "import csv\n",
    "\n",
    "# Define the GPT-2 model and tokenizer\n",
    "model_name = \"gpt2\"\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token  # Set pad_token to eos_token\n",
    "\n",
    "\n",
    "# Set device (GPU if available, otherwise CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Define a custom dataset to load sonnets from a CSV file\n",
    "class SonnetDataset(Dataset):\n",
    "    def __init__(self, csv_file, tokenizer, max_length=128):\n",
    "        self.sonnets = self.load_sonnets(csv_file)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def load_sonnets(self, csv_file):\n",
    "        sonnets = []\n",
    "        with open(csv_file, 'r', encoding='utf-8') as file:\n",
    "            reader = csv.reader(file)\n",
    "            for row in reader:\n",
    "                sonnet = row[0].strip()  # Assuming sonnet is in the first column\n",
    "                if sonnet:\n",
    "                    sonnets.append(sonnet)\n",
    "        return sonnets\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sonnets)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sonnet = self.sonnets[idx]\n",
    "        encoding = self.tokenizer(sonnet, return_tensors=\"pt\", truncation=True, max_length=self.max_length, padding=\"max_length\")\n",
    "        return {key: encoding[key][0] for key in encoding}\n",
    "\n",
    "def get_sonnet_dataset_path(language):\n",
    "    language_dict = {\n",
    "        \"English\": \"EnglishSonnets_plus50.csv\",\n",
    "        \"Italian\": \"Italian_Sonnets.csv\",\n",
    "        \"French\": \"French_Sonnets.csv\",\n",
    "        \"Hindi\": \"Hindi_Sonnets.csv\",\n",
    "    }\n",
    "\n",
    "    # Use the provided language to get the corresponding dataset path\n",
    "    dataset_path = language_dict.get(language, None)\n",
    "\n",
    "    if dataset_path is None:\n",
    "        raise ValueError(f\"No dataset path found for language: {language}\")\n",
    "\n",
    "    return dataset_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e548c879-8921-49c8-a072-a746a2b5cc34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For the requested language English, training model on EnglishSonnets_plus50.csv\n"
     ]
    }
   ],
   "source": [
    "# CHOOSE LANGUAGE DATASET TO TRAIN HERE\n",
    "\n",
    "language = \"English\"\n",
    "csv_file_path = get_sonnet_dataset_path(language)\n",
    "print(f\"For the requested language {language}, training model on {csv_file_path}\")\n",
    "\n",
    "# Set up your dataset and DataLoader\n",
    "sonnet_dataset = SonnetDataset(csv_file_path, tokenizer)\n",
    "train_loader = DataLoader(sonnet_dataset, batch_size=8, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e18dec3f-41e1-4fbe-bc05-5ae6c00a72a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 7/7 [00:01<00:00,  6.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 7/7 [00:00<00:00, 15.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 7/7 [00:00<00:00, 15.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 7/7 [00:00<00:00, 15.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 7/7 [00:00<00:00, 15.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 7/7 [00:00<00:00, 15.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 7/7 [00:00<00:00, 15.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 7/7 [00:00<00:00, 15.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 7/7 [00:00<00:00, 15.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 7/7 [00:00<00:00, 15.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 7/7 [00:00<00:00, 15.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 7/7 [00:00<00:00, 15.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 7/7 [00:00<00:00, 15.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 7/7 [00:00<00:00, 15.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 7/7 [00:00<00:00, 15.24it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('finetuned_model/tokenizer_config.json',\n",
       " 'finetuned_model/special_tokens_map.json',\n",
       " 'finetuned_model/vocab.json',\n",
       " 'finetuned_model/merges.txt',\n",
       " 'finetuned_model/added_tokens.json')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set up training parameters\n",
    "num_epochs = 15\n",
    "learning_rate = 5e-5\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.95)\n",
    "\n",
    "# Training loop\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "    for batch in tqdm(train_loader, desc=\"Training\"):\n",
    "        inputs = {key: batch[key].to(device) for key in batch}\n",
    "        outputs = model(**inputs, labels=inputs[\"input_ids\"])\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "    scheduler.step()\n",
    "\n",
    "# Save the trained model\n",
    "output_model_path = \"finetuned_model\"\n",
    "model.save_pretrained(output_model_path)\n",
    "tokenizer.save_pretrained(output_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c2f31a6c-3185-479d-9b48-198a8d789f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a sonnet using the fine-tuned model with TextGenerationConfig\n",
    "config = GPT2Config.from_pretrained(output_model_path)\n",
    "config.max_length = 200  # 14 lines with 10 syllables each\n",
    "config.num_return_sequences = 1  # Number of sequences to generate\n",
    "model = GPT2LMHeadModel.from_pretrained(output_model_path, config=config)\n",
    "model.to(device)  # Move the model back to the device\n",
    "\n",
    "import syllables\n",
    "\n",
    "def post_process_sonnet(generated_sonnet):\n",
    "    # Split the text into lines based on syllable count (10 syllables per line)\n",
    "    syllables_per_line = 10\n",
    "    words = generated_sonnet.split()\n",
    "    lines = [\" \".join(words[i:i+syllables_per_line]) for i in range(0, len(words), syllables_per_line)]\n",
    "\n",
    "    # Capitalize the first word of each line\n",
    "    lines = [line.capitalize() for line in lines]\n",
    "\n",
    "    # Ensure each line has exactly 10 syllables\n",
    "    for i, line in enumerate(lines):\n",
    "        current_syllables = sum(syllables.estimate(word) for word in line.split())\n",
    "        if current_syllables > syllables_per_line:\n",
    "            # Truncate words if the line has more than 10 syllables\n",
    "            words_in_line = line.split()\n",
    "            while current_syllables > syllables_per_line:\n",
    "                # Remove the last word until the line has exactly 10 syllables\n",
    "                last_word = words_in_line.pop()\n",
    "                current_syllables -= syllables.estimate(last_word)\n",
    "            lines[i] = \" \".join(words_in_line)\n",
    "        elif current_syllables < syllables_per_line:\n",
    "            # Pad words if the line has fewer than 10 syllables\n",
    "            lines[i] = line + \" \" + \" \".join([\"<PAD>\" for _ in range(syllables_per_line - current_syllables)])\n",
    "\n",
    "    # Filter out unwanted characters and remove empty lines\n",
    "    allowed_characters = set(\"abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ',.?! \")\n",
    "    lines = [\"\".join(c for c in line if c in allowed_characters) for line in lines]\n",
    "    lines = [line for line in lines if line]  # Remove empty lines\n",
    "\n",
    "    # Ensure the sonnet has exactly 14 lines\n",
    "    while len(lines) < 14:\n",
    "        lines.append(\" \".join([\"<PAD>\" for _ in range(syllables_per_line)]))\n",
    "\n",
    "    formatted_sonnet_q1 = \"\\n\".join(lines[:4])\n",
    "    formatted_sonnet_q2 = \"\\n\".join(lines[4:8])\n",
    "    formatted_sonnet_q3 = \"\\n\".join(lines[8:12])\n",
    "    formatted_sonnet_c = \"\\n\".join(lines[12:14])\n",
    "    formatted_sonnet = [formatted_sonnet_q1, \n",
    "                        formatted_sonnet_q2, \n",
    "                        formatted_sonnet_q3, \n",
    "                        formatted_sonnet_c]\n",
    "    \n",
    "    # Join the lines to form the sonnet\n",
    "    formatted_sonnet = \"\\n\\n\".join(formatted_sonnet)\n",
    "\n",
    "    # Remove any remaining <PAD> tokens\n",
    "    formatted_sonnet = formatted_sonnet.replace(\"<PAD>\", \"\")\n",
    "\n",
    "    return formatted_sonnet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "df2c7ab0-7921-4bd3-a295-fd5eb0b1d301",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sdandibh/.local/lib/python3.9/site-packages/transformers/generation/utils.py:1473: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n",
      "  warnings.warn(\n",
      "/home/sdandibh/.local/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.2` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/sdandibh/.local/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:386: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Formatted Sonnet:\n",
      "Love is a fair flower in the night, in\n",
      "Echoes, love finds its way a symphony\n",
      "Soft and kind. with quill in hand, the poet paints\n",
      "The sky with hues of the mind's alight, PAD\n",
      "\n",
      "         \n",
      "         \n",
      "         \n",
      "         \n",
      "\n",
      "         \n",
      "         \n",
      "         \n",
      "         \n",
      "\n",
      "         \n",
      "         \n"
     ]
    }
   ],
   "source": [
    "def generate_sonnet(theme_prompt):\n",
    "    # Generate a sonnet using the fine-tuned model\n",
    "    input_ids = tokenizer.encode(theme_prompt, return_tensors=\"pt\").to(device)\n",
    "    output = model.generate(input_ids, \n",
    "                            max_length=200, \n",
    "                            num_beams=5, \n",
    "                            no_repeat_ngram_size=2, \n",
    "                            top_k=50, \n",
    "                            top_p=0.95, \n",
    "                            temperature=0.2,\n",
    "                            pad_token_id=tokenizer.eos_token_id,\n",
    "                            attention_mask=torch.ones(input_ids.shape, device=device),)\n",
    "\n",
    "    # Decode and print the generated sonnet\n",
    "    generated_sonnet = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    formatted_sonnet = post_process_sonnet(generated_sonnet)\n",
    "    print(\"Formatted Sonnet:\")\n",
    "    print(formatted_sonnet)\n",
    "    \n",
    "# theme_prompt = \"Piacere di conoscerti\"\n",
    "theme_prompt = \"Love is a fair flower\"\n",
    "# theme_prompt = \"प्यार एक खूबसूरत फूल है\"\n",
    "generate_sonnet(theme_prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "94670d67-6f58-4d77-86e8-928092d82046",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_sonnet_structure(generated_sonnet):\n",
    "    # Define the expected structure of a sonnet\n",
    "    expected_line_count = 14\n",
    "    expected_syllables_per_line = 10\n",
    "\n",
    "    # Split the sonnet into lines\n",
    "    generated_lines = generated_sonnet.split('\\n')\n",
    "\n",
    "    # Evaluate line count\n",
    "    line_count_similarity = min(1, len(generated_lines) / expected_line_count)\n",
    "\n",
    "    # Evaluate syllable count per line\n",
    "    syllable_count_similarity = sum(\n",
    "        min(1, syllables.estimate(word) / expected_syllables_per_line)\n",
    "        for line in generated_lines\n",
    "        for word in line.split()\n",
    "    ) / len(generated_lines)\n",
    "\n",
    "    # Combine the metrics (adjust weights based on importance)\n",
    "    similarity_score = 0.6 * line_count_similarity + 0.4 * syllable_count_similarity\n",
    "\n",
    "    return similarity_score\n",
    "\n",
    "# # Evaluate the structure\n",
    "# structure_score = evaluate_sonnet_structure(generated_sonnet)\n",
    "# print(f\"Structure Score: {structure_score * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "104268fc-0e32-4c07-b596-42ee5480eb06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import textwrap\n",
    "import pronouncing\n",
    "\n",
    "def find_rhyming_word(word, reference_word):\n",
    "    # Find an exact rhyming word using the pronouncing library\n",
    "    rhymes = pronouncing.rhymes(reference_word)\n",
    "    return next((rhyme for rhyme in rhymes if rhyme != reference_word), word)\n",
    "\n",
    "def post_process_sonnet_rhyme(generated_sonnet):\n",
    "    # Split the text into lines based on syllable count (10 syllables per line)\n",
    "    syllables_per_line = 10\n",
    "    words = generated_sonnet.split()\n",
    "\n",
    "    # Wrap the text to the specified width (syllables_per_line * a reasonable value)\n",
    "    wrapped_lines = textwrap.wrap(\" \".join(words), width=syllables_per_line * 4)\n",
    "\n",
    "    # Ensure each line has exactly 10 syllables\n",
    "    for i in range(len(wrapped_lines)):\n",
    "        line_words = wrapped_lines[i].split()\n",
    "        current_syllables = sum(pronouncing.syllable_count(word) for word in line_words)\n",
    "\n",
    "        # Check if the line exceeds 10 syllables\n",
    "        if current_syllables > syllables_per_line:\n",
    "            remaining_syllables = syllables_per_line\n",
    "\n",
    "            # Iterate over the words in reverse to wrap the text\n",
    "            for j in range(len(line_words) - 1, -1, -1):\n",
    "                word = line_words[j]\n",
    "                word_syllables = pronouncing.syllable_count(word)\n",
    "\n",
    "                # If adding the current word exceeds the remaining syllables, move it to the next line\n",
    "                if remaining_syllables - word_syllables >= 0:\n",
    "                    remaining_syllables -= word_syllables\n",
    "                else:\n",
    "                    # Wrap the current word to the next line\n",
    "                    wrapped_lines[i] = \" \".join(line_words[:j])\n",
    "                    wrapped_lines.insert(i + 1, \" \".join(line_words[j:]))\n",
    "                    break\n",
    "\n",
    "    # Capitalize the first word of each line\n",
    "    wrapped_lines = [line.capitalize() for line in wrapped_lines]\n",
    "\n",
    "    # Ensure the sonnet has exactly 14 lines\n",
    "    while len(wrapped_lines) < 14:\n",
    "        wrapped_lines.append(\" \".join([\"<PAD>\" for _ in range(syllables_per_line)]))\n",
    "\n",
    "    # Rhyme scheme for a Shakespearean sonnet\n",
    "    rhyme_scheme = ['A', 'B', 'A', 'B', 'C', 'D', 'C', 'D', 'E', 'F', 'E', 'F', 'G', 'G']\n",
    "\n",
    "    # Initialize a dictionary to store the reference words for each rhyme group\n",
    "    reference_words = {}\n",
    "\n",
    "    # Make the last words rhyme in iambic pentameter according to the rhyme scheme\n",
    "    for i in range(14):\n",
    "        rhyme_group = rhyme_scheme[i]\n",
    "        last_word = wrapped_lines[i].split()[-1]\n",
    "\n",
    "        # Use the reference word if it exists for the rhyme group, otherwise, set it\n",
    "        reference_word = reference_words.get(rhyme_group, last_word)\n",
    "\n",
    "        # Find an exact rhyming word for the last word using the reference word\n",
    "        rhyme_word = find_rhyming_word(last_word, reference_word)\n",
    "\n",
    "        # Only change the word if it is not the first occurrence in the rhyme group\n",
    "        if last_word != reference_word:\n",
    "            wrapped_lines[i] = wrapped_lines[i].replace(last_word, rhyme_word)\n",
    "\n",
    "        # Store the reference word for the rhyme group\n",
    "        reference_words.setdefault(rhyme_group, last_word)\n",
    "\n",
    "    formatted_sonnet_q1 = \"\\n\".join(wrapped_lines[:4])\n",
    "    formatted_sonnet_q2 = \"\\n\".join(wrapped_lines[4:8])\n",
    "    formatted_sonnet_q3 = \"\\n\".join(wrapped_lines[8:12])\n",
    "    formatted_sonnet_c = \"\\n\".join(wrapped_lines[12:14])\n",
    "    formatted_sonnet = [formatted_sonnet_q1,\n",
    "                        formatted_sonnet_q2,\n",
    "                        formatted_sonnet_q3,\n",
    "                        formatted_sonnet_c]\n",
    "\n",
    "    # Join the lines to form the sonnet\n",
    "    formatted_sonnet = \"\\n\\n\".join(formatted_sonnet)\n",
    "\n",
    "    # Remove any remaining <PAD> tokens\n",
    "    formatted_sonnet = formatted_sonnet.replace(\"<PAD>\", \"\")\n",
    "\n",
    "    return formatted_sonnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0fcfd66-138f-4f18-85d8-78ec1f0153b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sonnet_from_prompt(theme_prompt, sonnet_number, language):\n",
    "    # Get language-specific tokenizer\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(language)\n",
    "\n",
    "    # Generate a sonnet using the fine-tuned model\n",
    "    input_ids = tokenizer.encode(theme_prompt, return_tensors=\"pt\").to(device)\n",
    "    output = model.generate(\n",
    "        input_ids,\n",
    "        max_length=200,\n",
    "        num_beams=20,\n",
    "        no_repeat_ngram_size=2,\n",
    "        top_k=50,\n",
    "        top_p=0.95,\n",
    "        temperature=0.2,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        attention_mask=torch.ones(input_ids.shape, device=device),\n",
    "    )\n",
    "\n",
    "    # Decode and print the generated sonnet\n",
    "    generated_sonnet = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    formatted_sonnet = post_process_sonnet_rhyme(generated_sonnet)\n",
    "\n",
    "    structure_score = evaluate_sonnet_structure(formatted_sonnet)\n",
    "\n",
    "    print(f\"Sonnet {sonnet_number} ({language}): \\n\")\n",
    "    print(formatted_sonnet + \"\\n\")\n",
    "    print(f\"Structure Score: {structure_score * 100:.2f}%\\n\")\n",
    "    print(\"_________________________________\")\n",
    "    return formatted_sonnet\n",
    "\n",
    "# # theme_prompt = \"How can you do this to me?\"\n",
    "# theme_prompt = \"Love is a fair flower in the rain\"\n",
    "# language = \"English\"  # Change to the desired language (\"Italian\", \"French\", \"Hindi\", etc.)\n",
    "# csv_file_path = get_sonnet_dataset_path(language)\n",
    "# sonnet_dataset = SonnetDataset(csv_file_path, tokenizer)\n",
    "# train_loader = DataLoader(sonnet_dataset, batch_size=8, shuffle=True)\n",
    "\n",
    "# # Generate sonnet for the chosen language\n",
    "# generated_sonnet = generate_sonnet_from_prompt(theme_prompt, \"Trial\", language)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af88ced-0d08-46bb-afc6-cf9725a055da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "import csv\n",
    "\n",
    "# Define the GPT-2 model\n",
    "model_name = \"gpt2\"\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "\n",
    "# Set device (GPU if available, otherwise CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Define a custom dataset to load sonnets from a CSV file\n",
    "class SonnetDataset(Dataset):\n",
    "    def __init__(self, csv_file, tokenizer, max_length=128):\n",
    "        self.sonnets = self.load_sonnets(csv_file)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def load_sonnets(self, csv_file):\n",
    "        sonnets = []\n",
    "        with open(csv_file, 'r', encoding='utf-8') as file:\n",
    "            reader = csv.reader(file)\n",
    "            for row in reader:\n",
    "                sonnet = row[0].strip()  # Assuming sonnet is in the first column\n",
    "                if sonnet:\n",
    "                    sonnets.append(sonnet)\n",
    "        return sonnets\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sonnets)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sonnet = self.sonnets[idx]\n",
    "        encoding = self.tokenizer(sonnet, return_tensors=\"pt\", truncation=True, max_length=self.max_length, padding=\"max_length\")\n",
    "        return {key: encoding[key][0] for key in encoding}\n",
    "\n",
    "def get_tokenizer(language):\n",
    "    language_dict = {\n",
    "        \"english\": \"gpt2\",\n",
    "        \"shakespeare\": \"gpt2\",\n",
    "        \"italian\": \"GroNLP/gpt2-small-italian\",\n",
    "        \"french\": \"dbddv01/gpt2-french-small\",\n",
    "        \"hindi\": \"surajp/gpt2-hindi\",\n",
    "        \"spanish\": \"datificate/gpt2-small-spanish\"\n",
    "    }\n",
    "\n",
    "    tokenizer_name = language_dict.get(language.lower(), \"gpt2\")\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(tokenizer_name)\n",
    "\n",
    "    # Add a padding token to the tokenizer\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "\n",
    "    return tokenizer\n",
    "\n",
    "def get_sonnet_dataset_path(language):\n",
    "    language_dict = {\n",
    "        \"english\": \"EnglishSonnets_plus50.csv\",\n",
    "        \"shakespeare\": \"shakespeare_sonnets.csv\",\n",
    "        \"italian\": \"Italian_Sonnets.csv\",\n",
    "        \"french\": \"French_Sonnets.csv\",\n",
    "        \"hindi\": \"Hindi_Sonnets.csv\",\n",
    "        \"spanish\": \"Italian_Sonnets.csv\"\n",
    "    }\n",
    "\n",
    "    # Use the provided language to get the corresponding dataset path\n",
    "    dataset_path = language_dict.get(language.lower(), None)\n",
    "\n",
    "    if dataset_path is None:\n",
    "        raise ValueError(f\"No dataset path found for language: {language}\")\n",
    "\n",
    "    return dataset_path\n",
    "\n",
    "def train_on_dataset(model, tokenizer, language, max_epochs=15):\n",
    "    csv_file_path = get_sonnet_dataset_path(language)\n",
    "    print(f\"For the requested language {language}, training model on {csv_file_path}\")\n",
    "\n",
    "    sonnet_dataset = SonnetDataset(csv_file_path, tokenizer)\n",
    "    train_loader = DataLoader(sonnet_dataset, batch_size=8, shuffle=True)\n",
    "\n",
    "#     optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
    "#     criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "#     for epoch in range(max_epochs):\n",
    "#         model.train()\n",
    "#         total_loss = 0.0\n",
    "\n",
    "#         for batch in tqdm(train_loader, desc=f\"Epoch {epoch + 1}/{max_epochs}\"):\n",
    "#             inputs = batch[\"input_ids\"].to(device)\n",
    "\n",
    "#             # Adjust the labels if needed based on your training setup\n",
    "#             labels = inputs.clone()\n",
    "\n",
    "#             optimizer.zero_grad()\n",
    "#             outputs = model(inputs, labels=labels)\n",
    "#             loss = criterion(outputs.logits.view(-1, model.config.vocab_size), labels.view(-1))\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "\n",
    "#             total_loss += loss.item()\n",
    "\n",
    "#         average_loss = total_loss / len(train_loader)\n",
    "#         print(f\"Epoch {epoch + 1}/{max_epochs}, Average Loss: {average_loss:.4f}\")\n",
    "\n",
    "    # Set up training parameters\n",
    "    num_epochs = 15\n",
    "    learning_rate = 5e-5\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.95)\n",
    "\n",
    "    # Training loop\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "        for batch in tqdm(train_loader, desc=\"Training\"):\n",
    "            inputs = {key: batch[key].to(device) for key in batch}\n",
    "            outputs = model(**inputs, labels=inputs[\"input_ids\"])\n",
    "            loss = outputs.loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "# Training the model on the specified language dataset\n",
    "language_to_train = \"french\"  # Change to the desired language (\"italian\", \"french\", \"hindi\", etc.)\n",
    "tokenizer_to_train = get_tokenizer(language_to_train)\n",
    "train_on_dataset(model, tokenizer_to_train, language_to_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67cbc619-762a-425b-b389-71aae76604da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating a sonnet in the specified language\n",
    "def generate_sonnet_from_prompt(theme_prompt, language, model, tokenizer):\n",
    "    # Generate a sonnet using the fine-tuned model\n",
    "    input_ids = tokenizer.encode(theme_prompt, return_tensors=\"pt\").to(device)\n",
    "    output = model.generate(input_ids, \n",
    "                            max_length=200, \n",
    "                            num_beams=20, \n",
    "                            no_repeat_ngram_size=2, \n",
    "                            top_k=50, \n",
    "                            top_p=0.95, \n",
    "                            temperature=0.2,\n",
    "                            pad_token_id=tokenizer.eos_token_id,\n",
    "                            attention_mask=torch.ones(input_ids.shape, device=device),)\n",
    "\n",
    "    # Decode and print the generated sonnet\n",
    "    generated_sonnet = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    formatted_sonnet = post_process_sonnet_rhyme(generated_sonnet)\n",
    "    \n",
    "    structure_score = evaluate_sonnet_structure(formatted_sonnet)\n",
    "    \n",
    "    print(f\"Generated Sonnet ({language}): \\n\")\n",
    "    print(formatted_sonnet + \"\\n\")\n",
    "    print(f\"Structure Score: {structure_score * 100:.2f}%\\n\")\n",
    "    print(\"_________________________________\")\n",
    "    return formatted_sonnet\n",
    "\n",
    "# Generating a sonnet in the specified language\n",
    "# theme_prompt = \"Le nuvole si tingono\"\n",
    "theme_prompt = \"J'adore, et je ne sais pas\"\n",
    "# theme_prompt = \"Love is a flower in the rain\"\n",
    "# theme_prompt = \"प्यार एक खूबसूरत फूल है\"\n",
    "\n",
    "generate_sonnet_from_prompt(theme_prompt, language_to_train, model, tokenizer_to_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea269b1e-b5b3-4b82-ad84-b7ffb07262e6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
